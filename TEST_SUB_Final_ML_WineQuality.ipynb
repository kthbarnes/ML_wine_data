{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Wine Quality Dataset"
      ],
      "metadata": {
        "id": "DWLJXJuRiYdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing basic libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from  sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "DtrnJNvojwOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/winequality-red.csv')"
      ],
      "metadata": {
        "id": "aFkwSElgsqIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an outline of my approach to the wine quality classification project\n",
        "\n",
        "I began with the wine quality dataset, which includes various physicochemical measurements (e.g., alcohol, volatile acidity, sulphates) as features and a â€œqualityâ€ score (ranging from 3 to 8) as the target label. Upon inspection, I confirmed that there were no missing values. However, I did find duplicate records and removed them\n",
        "\n",
        "\n",
        "\n",
        "Since the original quality scores span from 3 to 8, I decided to convert this into a binary classification task. I labeled wines with a quality score of 7 or higher as â€œgood,â€ and all others as â€œbad.â€ My thinking  was that, although scores of 5 or 6 may be somewhat subjective, a score of 7 or above generally indicates a broadly acceptable wine. After binarization, I discovered that only about 13% of the samples fell into the â€œgoodâ€ category, resulting in a class imbalance.\n",
        "\n",
        "\n",
        "I plotted histograms for each feature and observed that several of them exhibit skewed distributions. Given my initial plan to use tree-based models (which are generally robust to skewness), I chose not to apply any transformations at this stage.\n",
        "\n",
        "\n",
        "To identify features with the greatest impact, I generated a correlation matrix. This revealed that variables such as alcohol content, volatile acidity, and sulphates accounted for a large portion of the variance. Despite this insight, I did not drop or modify any features because i think decision tree decision trees can handle and will spilit accordinly\n",
        "\n",
        "\n",
        "\n",
        "After splitting the data into training and test sets, I applied Synthetic Minority Overâ€sampling Technique (SMOTE) to the training data to balance the â€œgoodâ€ and â€œbadâ€ classes (only on training). Once SMOTE had been performed, I standardized all features (mean = 0, standard deviation = 1) before proceeding to modelling.\n",
        "In my first modelling pipeline, I trained three classifiers:\n",
        "â€¢\tDecision Tree\n",
        "â€¢\tRandom Forest\n",
        "â€¢\tAdaBoost\n",
        "\n",
        "\n",
        "I used F1 score as my primary evaluation metric because the class distribution was highly imbalanced. Unfortunately, each of these models achieved an F1 score of only approximately 0.50 on the test set, which I consider unsatisfactory.\n",
        "\n",
        "\n",
        "Hoping to improve performance, I created a second pipeline that began with principal component analysis (PCA) for dimensionality reduction. My intention was that reducing dimensionality might indirectly address skewness without requiring explicit transformations for each feature. On the resulting principal components, I then trained:\n",
        "â€¢\tk-Nearest Neighbors (KNN)\n",
        "â€¢\tSupport Vector Classifier (SVC)\n",
        "â€¢\tGradient Boosting Classifier\n",
        "\n",
        "\n",
        "Despite this change, the F1 scores on the test set again hovered around 0.50, indicating that the adjustments did not substantially improve performance.\n",
        "\n",
        "\n",
        "At this point, I am not seeing any meaning results and would like to seek your guidance on it. My initial thought process was to train a bunch of models and select the best performing one and then tune it for better results. Would like to hear your thoughts on my approach and where i could improve and whether my approach was correct or not.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nDi6VgD9BVEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "Wj4EnclawrTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "23oJFhwltn0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "edpGl_-nw4lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()\n",
        "# no null values found"
      ],
      "metadata": {
        "id": "r0c1fE6m1Fpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "Y9NXMZQEuQTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "xNWDPMI2uSIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hist plot to understand distribution\n",
        "df.hist(bins=10, figsize=(10, 10))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jM4OE74YyYwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "_daYtTEzz3jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "240 values are duplicates, removing them"
      ],
      "metadata": {
        "id": "A0B9USjE46NU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop_duplicates()\n",
        "#check current shape of dataset\n",
        "df.shape"
      ],
      "metadata": {
        "id": "n163PiiT4_82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "df.hist(bins=10, figsize=(10, 10))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h_n5YBaq5T5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the mean and std deviation have changed but only slightly\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "0m09WDGU5gwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar plot for quality vs features\n",
        "fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(10,10)) #create a figure (size can be anything) and an gird of axes 4x3\n",
        "axes = axes.flatten() # convert 2d to 1d so no need to do matrix like iteration\n",
        "\n",
        "features = df.columns.tolist() #convert columns into a list so that they can be iterated\n",
        "features.remove('quality') # drop target label\n",
        "\n",
        "for i, col in enumerate(features):\n",
        "    sns.barplot(x='quality', y=col, data=df, ax=axes[i])\n",
        "    axes[i].set_title(f'{col} vs Quality')\n",
        "    axes[i].set_xlabel('Quality')\n",
        "    axes[i].set_ylabel(col)\n",
        "\n",
        "plt.tight_layout() #adjusts the plot, prevents overlapping\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YkZVeuCBA2mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Box plot for quality vs features\n",
        "fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(10, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "features = df.columns.tolist()\n",
        "features.remove('quality')\n",
        "\n",
        "for i, col in enumerate(features):\n",
        "    # Changed sns.barplot to sns.boxplot\n",
        "    sns.boxplot(x='quality', y=col, data=df, ax=axes[i])\n",
        "    axes[i].set_title(f'{col} vs Quality')\n",
        "    axes[i].set_xlabel('Quality')\n",
        "    axes[i].set_ylabel(col)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "10iCK07LVznv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-wxjoy8qbBG"
      },
      "source": [
        "# constructing a heatmap to understand the correlation between the columns\n",
        "correlation = df.corr()\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(correlation, cbar=True, square=True, fmt = '.2f', annot = True, annot_kws={'size':8}, cmap = 'Blues')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Get the absolute correlation values with 'quality'\n",
        "quality_correlation = correlation['quality'].abs().sort_values(ascending=False)\n",
        "\n",
        "# Remove the correlation of 'quality' with itself\n",
        "quality_correlation = quality_correlation.drop('quality')\n",
        "\n",
        "# Select the top N most important features (you can adjust N)\n",
        "n = 10\n",
        "most_important_features = quality_correlation.head(n)\n",
        "\n",
        "print(\"Most important features based on correlation with quality:\")\n",
        "print(most_important_features)\n",
        "\n",
        "# Create a pie chart of the top most important features\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(most_important_features, labels=most_important_features.index, autopct='%1.1f%%', startangle=140)\n",
        "plt.title(f'Top {n} Most Important Features for Quality (Correlation)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9JYW2X6lB_37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the distribution of quality column\n",
        "df['quality'].value_counts()\n"
      ],
      "metadata": {
        "id": "3wBILL6uxa8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Visualize the distribution of quality\n",
        "plt.figure(figsize=(5, 5))\n",
        "sns.countplot(x='quality', data=df)\n",
        "plt.title('Distribution of Wine Quality')\n",
        "plt.xlabel('Quality')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Print the count of each quality value in a table\n",
        "quality_counts = df['quality'].value_counts().sort_index()\n",
        "quality_counts = quality_counts[(quality_counts.index >= 3) & (quality_counts.index <= 8)]\n",
        "\n",
        "print(\"Quality Value Counts:\")\n",
        "print(quality_counts.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "total_count = quality_counts.sum()\n",
        "print(f\"\\nTotal Count: {total_count}\")"
      ],
      "metadata": {
        "id": "mVR83zI8eeXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#binarizing the target variable as Good (1) or bad (0)\n",
        "# good based on whether it is orgianlly 7 or above\n",
        "\n",
        "df['quality'] = [1 if x>=7 else 0 for x in df['quality']]\n",
        "df['quality'].value_counts()"
      ],
      "metadata": {
        "id": "7_Mwk0qxgcyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the countplot of quality values\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "sns.countplot(x='quality', data=df)\n",
        "plt.title('Distribution of Binarized Wine Quality (0: Bad, 1: Good)')\n",
        "plt.xlabel('Quality (0: Bad, 1: Good)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SnTlyvH3molA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "will need to do imbalance handling but before that train test split"
      ],
      "metadata": {
        "id": "UeREV22doAEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# naming convention X for features and lower case y for targets\n",
        "X = df.drop(\"quality\", axis=1)\n",
        "y = df[\"quality\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "RmUHwZrvnQY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "y_train.value_counts()"
      ],
      "metadata": {
        "id": "MkyD-bcK0SW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# APPLY LOG TRANSFORMATION TO SKEWED FEATURES\n",
        "# List of skewed features identified from the histograms\n",
        "skewed_features = [\n",
        "    'fixed acidity', 'volatile acidity', 'residual sugar', 'chlorides',\n",
        "    'free sulfur dioxide', 'total sulfur dioxide', 'sulphates', 'alcohol'\n",
        "]\n",
        "\n",
        "# Apply log transformation (np.log1p handles zero values gracefully)\n",
        "for col in skewed_features:\n",
        "    X_train[col] = np.log1p(X_train[col])\n",
        "    X_test[col] = np.log1p(X_test[col])"
      ],
      "metadata": {
        "id": "fJ0h_pALzh3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  plot the histogram after the log transformation\n",
        "\n",
        "\n",
        "# Plot histograms after log transformation\n",
        "X_train.hist(bins=10, figsize=(5, 5))\n",
        "plt.suptitle('Histograms of Features After Log Transformation (Training Data)', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mW98Qwqe0JSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardize the feature data\n",
        "scaler = StandardScaler()\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# Scale the data and immediately wrap it in a DataFrame to preserve feature names\n",
        "X_train_scaled_df = pd.DataFrame(scaler.fit_transform(X_train), columns=feature_names)\n",
        "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=feature_names)\n",
        "\n",
        "print(\"Class distribution before SMOTE:\\n\", y_train.value_counts())"
      ],
      "metadata": {
        "id": "I-oaKXklv2NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Descriptive statistics of scaled training data:\")\n",
        "X_train_scaled_df.describe()"
      ],
      "metadata": {
        "id": "EU8GcqsGwLZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# handling imbalance using smote\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Apply SMOTE to the SCALED training data\n",
        "# SMOTE is only applied to the training set to prevent the model from seeing synthetic\n",
        "# versions of the test data.\n",
        "# Because the input is a DataFrame, SMOTE will also output a DataFrame\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_scaled, y_train_smote = smote.fit_resample(X_train_scaled_df, y_train)\n",
        "\n",
        "# The naming convention will remain the same here on but the order is now correct"
      ],
      "metadata": {
        "id": "t36O3Wm7tfPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print the count of the data so that we know smote has worked\n",
        "\n",
        "print(\"Count of the target variable after SMOTE:\")\n",
        "y_train_smote.value_counts()"
      ],
      "metadata": {
        "id": "8VGoEae-xPQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# commenting out this part since the order was wrong\n",
        "#scaler = StandardScaler()\n",
        "#X_train_scaled = scaler.fit_transform(X_train_smote)\n",
        "#X_test_scaled = scaler.transform(X_test)  # Important: transform only for the test data so that there is no leak"
      ],
      "metadata": {
        "id": "vpTZWjGot8pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix , accuracy_score , precision_score, recall_score, f1_score\n",
        "\n",
        "#dafult spillter is best but without random_state set the output is different - tie breaking is random !\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train_scaled, y_train_smote)\n",
        "y_pred_dt = dt.predict(X_test_scaled)\n",
        "\n",
        "print(\"Decision Tree classifier\")\n",
        "print(\"confusion matrix (0  1)\") # remember scikit learn uses 0 1 by default\n",
        "print(confusion_matrix(y_test, y_pred_dt))\n",
        "# look at the class 1 metrics\n",
        "print(classification_report(y_test, y_pred_dt))\n",
        "#printing all the main metrics for quick reference\n",
        "print(\"\\nImportant Model Evaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_dt):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_dt):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred_dt):.4f}\")"
      ],
      "metadata": {
        "id": "fOEdE3AZuUe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for imbalance classes accuracy is not enough we need to look at the precision an recall and specifically the F1 score since that i high when both recall and precision are high - so a better indicator for our use case\n"
      ],
      "metadata": {
        "id": "oq3LreI3-DP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# random forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train_scaled, y_train_smote)\n",
        "y_pred_rf =rf.predict(X_test_scaled)\n",
        "print(\"Random Forest\")\n",
        "print(confusion_matrix(y_test, y_pred_rf))\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "print(\"\\nImportant Model Evaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_rf):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_rf):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred_rf):.4f}\")\n"
      ],
      "metadata": {
        "id": "DVW00CCsACjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "# Assuming X_train_scaled, y_train_smote, X_test_scaled, y_test are already defined\n",
        "# (These would come from your data loading and preprocessing steps)\n",
        "\n",
        "# --- Hyperparameter Tuning for Random Forest ---\n",
        "\n",
        "# Define the parameter distribution for RandomizedSearchCV\n",
        "# Using distributions for continuous/integer parameters for better exploration\n",
        "param_grid_rf_conservative = {\n",
        "    'n_estimators': [100, 150], # Only two values - the low and high from your original randint\n",
        "    'max_features': ['sqrt', 0.8], # Focus on 'sqrt' (default-like) and a slightly higher fraction\n",
        "    'max_depth': [15, 30], # Two mid-range values from your original 10-50 range\n",
        "    'min_samples_split': [2, 10], # Default and a more regularized value\n",
        "    'min_samples_leaf': [1, 5], # Default and a more regularized value\n",
        "    'bootstrap': [True], # True is almost always preferred for Random Forests. Remove False for speed.\n",
        "    'class_weight': ['balanced'] # Prioritize 'balanced' given your imbalanced data context. Remove None for speed.\n",
        "}\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "rf_base = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "# We target 'f1' as the scoring metric due to class imbalance\n",
        "# n_iter: Number of parameter settings that are sampled. Increase for more exhaustive search.\n",
        "# cv: Number of folds for cross-validation\n",
        "# n_jobs: -1 means use all available processors\n",
        "random_search_rf = RandomizedSearchCV(\n",
        "    estimator=rf_base,\n",
        "    param_distributions=param_distributions_rf,\n",
        "    n_iter=100, # Increased iterations for better exploration\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV to the SMOTE-treated training data\n",
        "print(\"\\n--- Starting Random Forest Hyperparameter Tuning (Randomized Search) ---\")\n",
        "random_search_rf.fit(X_train_scaled, y_train_smote)\n",
        "\n",
        "print(\"\\n--- Tuning Complete ---\")\n",
        "print(\"Best parameters found for Random Forest:\")\n",
        "print(random_search_rf.best_params_)\n",
        "print(f\"Best F1-Score on training data (cross-validated): {random_search_rf.best_score_:.4f}\")\n",
        "\n",
        "# Get the best Random Forest model\n",
        "best_rf_model = random_search_rf.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "print(\"\\n--- Evaluating Best Tuned Random Forest Model on Test Set ---\")\n",
        "y_pred_rf_tuned = best_rf_model.predict(X_test_scaled)\n",
        "\n",
        "print(\"Tuned Random Forest\")\n",
        "print(confusion_matrix(y_test, y_pred_rf_tuned))\n",
        "print(classification_report(y_test, y_pred_rf_tuned))\n",
        "\n",
        "print(\"\\nImportant Model Evaluation Metrics (Tuned Random Forest):\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf_tuned):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_rf_tuned):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_rf_tuned):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred_rf_tuned):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oofcUiGCMDY-",
        "outputId": "2c558a52-45a2-4ebb-bb67-6527a62827c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Random Forest Hyperparameter Tuning (Randomized Search) ---\n",
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "# AdaBoost Classifier\n",
        "ab = AdaBoostClassifier(random_state=42)\n",
        "ab.fit(X_train_scaled, y_train_smote)\n",
        "y_pred_ab = ab.predict(X_test_scaled)\n",
        "\n",
        "print(\"ðŸ“Œ AdaBoost Classifier\")\n",
        "print(confusion_matrix(y_test, y_pred_ab))\n",
        "print(classification_report(y_test, y_pred_ab))\n",
        "\n",
        "print(\"\\nImportant Model Evaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_ab):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_ab):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_ab):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred_ab):.4f}\")\n"
      ],
      "metadata": {
        "id": "0A6VrmnGI6ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA TRANSFORM\n"
      ],
      "metadata": {
        "id": "qPbMIK8-OjpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=0.90, random_state=42)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)"
      ],
      "metadata": {
        "id": "nhJeyaadKWWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# K-Nearest Neighbors with PCA transformed data\n",
        "knc = KNeighborsClassifier()\n",
        "knc.fit(X_train_scaled, y_train_smote)\n",
        "y_pred_knc_pca = knc.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\n K-Nearest Neighbors with PCA transformed data\")\n",
        "print(\"confusion matrix (0  1)\")\n",
        "print(confusion_matrix(y_test, y_pred_knc_pca))\n",
        "print(classification_report(y_test, y_pred_knc_pca))\n",
        "\n",
        "print(\"\\nImportant Model Evaluation Metrics (K-Nearest Neighbors with PCA):\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_knc_pca):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_knc_pca):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_knc_pca):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred_knc_pca):.4f}\")\n"
      ],
      "metadata": {
        "id": "eRlLrVTLO8K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: train and evaluate knn on the regular scaled dataset\n",
        "\n",
        "# Train and evaluate KNN on the regular scaled dataset (without PCA)\n",
        "knc_regular = KNeighborsClassifier()\n",
        "knc_regular.fit(X_train_scaled, y_train_smote) # Use scaled data without PCA\n",
        "y_pred_knc_regular = knc_regular.predict(X_test_scaled) # Use scaled test data without PCA\n",
        "\n",
        "print(\"\\n K-Nearest Neighbors on Regular Scaled Data\")\n",
        "print(\"confusion matrix (0  1)\")\n",
        "print(confusion_matrix(y_test, y_pred_knc_regular))\n",
        "print(classification_report(y_test, y_pred_knc_regular))\n",
        "\n",
        "print(\"\\nImportant Model Evaluation Metrics (K-Nearest Neighbors on Regular Scaled Data):\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_knc_regular):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_knc_regular):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_knc_regular):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred_knc_regular):.4f}\")"
      ],
      "metadata": {
        "id": "O4Dcl8f64L6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Support Vector Classifier with PCA transformed data\n",
        "svc = SVC(random_state=42)\n",
        "svc.fit(X_train_pca, y_train_smote)\n",
        "y_pred_svc_pca = svc.predict(X_test_pca)\n",
        "\n",
        "print(\"\\nSupport Vector Classifier with PCA transformed data\")\n",
        "print(\"confusion matrix (0  1)\")\n",
        "print(confusion_matrix(y_test, y_pred_svc_pca))\n",
        "print(classification_report(y_test, y_pred_svc_pca))\n",
        "\n",
        "print(\"\\nImportant Model Evaluation Metrics (Support Vector Classifier with PCA):\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_svc_pca):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_svc_pca):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_svc_pca):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred_svc_pca):.4f}\")"
      ],
      "metadata": {
        "id": "v6L06Y5soCtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: apply svc on normal scaled data and print the metric like before\n",
        "\n",
        "# Support Vector Classifier on regular scaled data (without PCA)\n",
        "svc_regular = SVC(random_state=42)\n",
        "svc_regular.fit(X_train_scaled, y_train_smote) # Use scaled data without PCA\n",
        "y_pred_svc_regular = svc_regular.predict(X_test_scaled) # Use scaled test data without PCA\n",
        "\n",
        "print(\"\\nSupport Vector Classifier on Regular Scaled Data\")\n",
        "print(\"confusion matrix (0  1)\")\n",
        "print(confusion_matrix(y_test, y_pred_svc_regular))\n",
        "print(classification_report(y_test, y_pred_svc_regular))\n",
        "\n",
        "print(\"\\nImportant Model Evaluation Metrics (Support Vector Classifier on Regular Scaled Data):\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_svc_regular):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_svc_regular):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_svc_regular):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred_svc_regular):.4f}\")"
      ],
      "metadata": {
        "id": "1bkUsOQR81uB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply gradient boost to data without pca and report metrics\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Gradient Boosting Classifier without PCA\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "gb.fit(X_train_scaled, y_train_smote)\n",
        "y_pred_gb = gb.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\nGradient Boosting Classifier without PCA\")\n",
        "print(\"confusion matrix (0  1)\")\n",
        "print(confusion_matrix(y_test, y_pred_gb))\n",
        "print(classification_report(y_test, y_pred_gb))\n",
        "\n",
        "print(\"\\nImportant Model Evaluation Metrics (Gradient Boosting without PCA):\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_gb):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_gb):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_gb):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred_gb):.4f}\")\n"
      ],
      "metadata": {
        "id": "1dclyYXmoqIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model names\n",
        "model_names = ['Decision Tree', 'Random Forest', 'AdaBoost',\n",
        "               'K-Nearest Neighbors (PCA)', 'Support Vector Classifier (PCA)',\n",
        "               'Gradient Boosting']\n",
        "\n",
        "# Collect evaluation metrics\n",
        "f1_scores = [\n",
        "    f1_score(y_test, y_pred_dt),\n",
        "    f1_score(y_test, y_pred_rf),\n",
        "    f1_score(y_test, y_pred_ab),\n",
        "    f1_score(y_test, y_pred_knc_pca),\n",
        "    f1_score(y_test, y_pred_svc_pca),\n",
        "    f1_score(y_test, y_pred_gb)\n",
        "]\n",
        "\n",
        "recall_scores = [\n",
        "    recall_score(y_test, y_pred_dt),\n",
        "    recall_score(y_test, y_pred_rf),\n",
        "    recall_score(y_test, y_pred_ab),\n",
        "    recall_score(y_test, y_pred_knc_pca),\n",
        "    recall_score(y_test, y_pred_svc_pca),\n",
        "    recall_score(y_test, y_pred_gb)\n",
        "]\n",
        "\n",
        "precision_scores = [\n",
        "    precision_score(y_test, y_pred_dt),\n",
        "    precision_score(y_test, y_pred_rf),\n",
        "    precision_score(y_test, y_pred_ab),\n",
        "    precision_score(y_test, y_pred_knc_pca),\n",
        "    precision_score(y_test, y_pred_svc_pca),\n",
        "    precision_score(y_test, y_pred_gb)\n",
        "]\n",
        "\n",
        "accuracy_scores = [\n",
        "    accuracy_score(y_test, y_pred_dt),\n",
        "    accuracy_score(y_test, y_pred_rf),\n",
        "    accuracy_score(y_test, y_pred_ab),\n",
        "    accuracy_score(y_test, y_pred_knc_pca),\n",
        "    accuracy_score(y_test, y_pred_svc_pca),\n",
        "    accuracy_score(y_test, y_pred_gb)\n",
        "]\n",
        "\n",
        "# Create a DataFrame with all metrics\n",
        "results_df = pd.DataFrame({\n",
        "    'Model': model_names,\n",
        "    'Accuracy': accuracy_scores,\n",
        "    'Precision': precision_scores,\n",
        "    'Recall': recall_scores,\n",
        "    'F1-Score': f1_scores\n",
        "})\n",
        "\n",
        "# Rank by F1-Score\n",
        "results_df_ranked = results_df.sort_values(by='F1-Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display table in terminal\n",
        "print(\"Model Performance Comparison (Ranked by F1-Score):\")\n",
        "print(results_df_ranked.to_markdown(index=False, floatfmt=\".4f\"))\n",
        "\n",
        "# Melt the DataFrame for easier Seaborn plotting\n",
        "melted_df = results_df_ranked.melt(id_vars='Model',\n",
        "                                   value_vars=['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
        "                                   var_name='Metric',\n",
        "                                   value_name='Score')\n",
        "\n",
        "# Set up a 2x2 subplot grid for the metrics\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "palettes = ['inferno', 'cividis', 'magma', 'viridis']\n",
        "\n",
        "# Loop through and create one barplot per metric\n",
        "for ax, metric, palette in zip(axes.flat, metrics, palettes):\n",
        "    sns.barplot(data=melted_df[melted_df['Metric'] == metric],\n",
        "                y='Model', x='Score', hue='Model', palette=palette, legend=False, ax=ax)\n",
        "\n",
        "    ax.set_title(f'{metric} Comparison')\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_xlabel(metric)\n",
        "    ax.set_ylabel('')\n",
        "\n",
        "plt.suptitle('Model Comparison Across Metrics (Ranked by F1-Score)', fontsize=14)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7FeQCUNs6cgS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}